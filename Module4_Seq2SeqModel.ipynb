{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Module4_Seq2SeqModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhairavi-shah/AbstractiveUserReviewConsolidation/blob/master/Module4_Seq2SeqModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ea3b1ec7-5fc1-43de-e807-ace33cc0795d",
        "id": "W8argiSZ1lCj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#Initialise drive configurations\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/PROJECT | S7-S8/Colab Notebooks/')\n",
        "\n",
        "#import custom attention layer\n",
        "from attention import AttentionLayer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5WeMBqyULqN",
        "colab_type": "code",
        "outputId": "272725e1-3827-4966-c4e3-5b855f2192f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Import modules\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import re\n",
        "import pickle\n",
        "from bs4 import BeautifulSoup\n",
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from google.colab import files\n",
        "import warnings\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67Vy2AHnUON9",
        "colab_type": "code",
        "outputId": "2e32b135-67dc-499a-9793-89d9669b76c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "%%time\n",
        "#Read data\n",
        "data = pd.read_csv(r\"/content/gdrive/My Drive/PROJECT | S7-S8/Data/training_seq2seq.csv\")\n",
        "print(data.columns)\n",
        "print(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['Unnamed: 0', 'Id', 'ProductId', 'UserId', 'ProfileName',\n",
            "       'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Score', 'Time',\n",
            "       'Summary', 'Text'],\n",
            "      dtype='object')\n",
            "        Unnamed: 0  ...                                                                                                                                                                                                     Text\n",
            "0           329412  ...  my 12 year old sheltie has chronic brochotitis, been on meds for three years, these help with the coughing fits at night. so I like them!!!<br /> one bite and both my dogs are addicted!!! be warne...\n",
            "1           329413  ...  These are genuine Greenies product, not a knockoff.  My dogs love them!  It is their favorite treat.  I have 2 small dogs, both under 10 lbs. and the Teenie size is perfect for them.  I paid $22.3...\n",
            "2           329414  ...                   Our dogs love Greenies, but of course, which doggies don't?  I bought this for my dashchund and minpin, and it's perfect!  A great price for a great product.  Who could ask for more.\n",
            "3           329415  ...                                                                                   What can I say, dogs love greenies. They begg for them all the time. They always sit by the cupboard and ask for more.\n",
            "4           329416  ...                              This review is for a box of Greenies Lite for my dog. The package came quickly and was packaged appropriately. I was very satisfied with this purchase and with the seller.\n",
            "...            ...  ...                                                                                                                                                                                                      ...\n",
            "131727      165636  ...  I was a huge fan of canidae, but beginning about a month ago, the company changed the formula to cheaper raw materials, smaller bags, and worst of all outsourced manufacturing to Diamond pet foods...\n",
            "131728      165637  ...                                           My dogs enjoy this food and seem lively and excited to eat it. Very happy also that it's not made by Menu Foods - peace of mind factor is high. Great product!\n",
            "131729      165638  ...                                                                            I was happy to see that I could order this product online and have it shipped to my home.  It is difficult to find in stores.\n",
            "131730      165639  ...  I bought this food for my adult male chiweenie and two female chiweeniepom puppies. I researched for a while before finally deciding to get this particular brand. I'm glad I did get this brand. My...\n",
            "131731      165640  ...  Not only does my dog love the food but it got here super quick and just in time for me to have enough food for my dog for the next month in a half or so. I have an english bulldog and it keeps him...\n",
            "\n",
            "[131732 rows x 11 columns]\n",
            "CPU times: user 698 ms, sys: 127 ms, total: 825 ms\n",
            "Wall time: 1.54 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl9vv8FAUZ7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dropping duplicates and na values\n",
        "data.drop_duplicates(subset=['Text'],inplace=True)\n",
        "data.dropna(axis=0,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8mLAABEUchu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJWvPeQ0Ugcz",
        "colab_type": "code",
        "outputId": "c670b8fb-4ef3-4b48-eb2a-52a75644aa7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "# Preprocessing function\n",
        "def text_cleaner(text,num):\n",
        "    newString = text.lower()\n",
        "    newString = BeautifulSoup(newString, \"lxml\").text\n",
        "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
        "    newString = re.sub('\"','', newString)\n",
        "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
        "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
        "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
        "    newString = re.sub('[m]{2,}', 'mm', newString)\n",
        "    if(num==0):\n",
        "        tokens = [w for w in newString.split() if not w in stop_words]\n",
        "    else:\n",
        "        tokens=newString.split()\n",
        "    long_words=[]\n",
        "    for i in tokens:\n",
        "        if len(i)>1:                                                 #removing short word\n",
        "            long_words.append(i)   \n",
        "    return (\" \".join(long_words)).strip()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayVmuV1uUny5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#call text_cleaner function to preprocess both review and summary for training\n",
        "cleaned_text = []\n",
        "for t in data['Text']:\n",
        "    cleaned_text.append(text_cleaner(t,0)) \n",
        "\n",
        "cleaned_summary = []\n",
        "for t in data['Summary']:\n",
        "    cleaned_summary.append(text_cleaner(t,1))\n",
        "\n",
        "data['cleaned_text']=cleaned_text\n",
        "data['cleaned_summary']=cleaned_summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K237fFRtUxIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#drop empty rows\n",
        "data.replace('', np.nan, inplace=True)\n",
        "data.dropna(axis=0,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCBdU3e6Uzd6",
        "colab_type": "code",
        "outputId": "21d16be6-71b6-4da2-99eb-2caf5af177c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#to fix max length of the sequence\n",
        "cnt=0\n",
        "for i in data['cleaned_summary']:\n",
        "    if(len(i.split())<=15):\n",
        "        cnt=cnt+1\n",
        "print(cnt/len(data['cleaned_summary']))\n",
        "\n",
        "max_text_len=50\n",
        "max_summary_len=15"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9979647591631676\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gSj5XNyVbVY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#selecting reviews with length equal to or less than max_len\n",
        "cleaned_text =np.array(data['cleaned_text'])\n",
        "cleaned_summary=np.array(data['cleaned_summary'])\n",
        "\n",
        "short_text=[]\n",
        "short_summary=[]\n",
        "\n",
        "for i in range(len(cleaned_text)):\n",
        "    if(len(cleaned_summary[i].split())<=max_summary_len and len(cleaned_text[i].split())<=max_text_len):\n",
        "        short_text.append(cleaned_text[i])\n",
        "        short_summary.append(cleaned_summary[i])\n",
        "        \n",
        "df=pd.DataFrame({'text':short_text,'summary':short_summary})\n",
        "\n",
        "#adding start and end tokens to the data\n",
        "df['summary'] = df['summary'].apply(lambda x : 'sostok '+ x + ' eostok')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FZFbLjnVfpI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#split train and test data\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_tr,x_val,y_tr,y_val=train_test_split(np.array(df['text']),np.array(df['summary']),test_size=0.1,random_state=0,shuffle=True) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TChU5J1rVh6i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#prepare a tokenizer for reviews on training data\n",
        "x_tokenizer = Tokenizer() \n",
        "x_tokenizer.fit_on_texts(list(x_tr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ni-yQLEsVlK0",
        "colab_type": "code",
        "outputId": "a19e8b0d-e876-4942-ff3a-60c1db8eb007",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "#find proportion of rare words in the data\n",
        "thresh=4\n",
        "\n",
        "cnt=0\n",
        "tot_cnt=0\n",
        "freq=0\n",
        "tot_freq=0\n",
        "\n",
        "for key,value in x_tokenizer.word_counts.items():\n",
        "    tot_cnt=tot_cnt+1\n",
        "    tot_freq=tot_freq+value\n",
        "    if(value<thresh):\n",
        "        cnt=cnt+1\n",
        "        freq=freq+value\n",
        "    \n",
        "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
        "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "% of rare words in vocabulary: 63.79420129897695\n",
            "Total Coverage of rare words: 2.343126767713395\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YVBvlK7Vnoa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#prepare a tokenizer for reviews on training data\n",
        "x_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
        "x_tokenizer.fit_on_texts(list(x_tr))\n",
        "\n",
        "#convert text sequences into integer sequences\n",
        "x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr)\n",
        "x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n",
        "\n",
        "#padding zero upto maximum length\n",
        "x_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
        "x_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
        "\n",
        "#size of vocabulary ( +1 for padding token)\n",
        "x_voc   =  x_tokenizer.num_words + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA1lt-LfVp7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#prepare a tokenizer for summary on training data\n",
        "y_tokenizer = Tokenizer()   \n",
        "y_tokenizer.fit_on_texts(list(y_tr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q8Uf9wwVuSf",
        "colab_type": "code",
        "outputId": "2c2ce7e7-1e14-4c1e-ae0e-d2371124163b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "#find proportion of rare words in the data\n",
        "thresh=6\n",
        "\n",
        "cnt=0\n",
        "tot_cnt=0\n",
        "freq=0\n",
        "tot_freq=0\n",
        "\n",
        "for key,value in y_tokenizer.word_counts.items():\n",
        "    tot_cnt=tot_cnt+1\n",
        "    tot_freq=tot_freq+value\n",
        "    if(value<thresh):\n",
        "        cnt=cnt+1\n",
        "        freq=freq+value\n",
        "    \n",
        "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
        "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "% of rare words in vocabulary: 74.83129935391241\n",
            "Total Coverage of rare words: 4.478674780939875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvhyAvbhVwmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#prepare a tokenizer for summary on training data\n",
        "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
        "y_tokenizer.fit_on_texts(list(y_tr))\n",
        "\n",
        "#convert text sequences into integer sequences\n",
        "y_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) \n",
        "y_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \n",
        "\n",
        "#padding zero upto maximum length\n",
        "y_tr    =   pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\n",
        "y_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
        "\n",
        "#size of vocabulary\n",
        "y_voc  =   y_tokenizer.num_words +1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMkEbgjCV0kM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#delete rows with only start and end tokens\n",
        "ind=[]\n",
        "for i in range(len(y_tr)):\n",
        "    cnt=0\n",
        "    for j in y_tr[i]:\n",
        "        if j!=0:\n",
        "            cnt=cnt+1\n",
        "    if(cnt==2):\n",
        "        ind.append(i)\n",
        "\n",
        "y_tr=np.delete(y_tr,ind, axis=0)\n",
        "x_tr=np.delete(x_tr,ind, axis=0)\n",
        "\n",
        "ind=[]\n",
        "for i in range(len(y_val)):\n",
        "    cnt=0\n",
        "    for j in y_val[i]:\n",
        "        if j!=0:\n",
        "            cnt=cnt+1\n",
        "    if(cnt==2):\n",
        "        ind.append(i)\n",
        "\n",
        "y_val=np.delete(y_val,ind, axis=0)\n",
        "x_val=np.delete(x_val,ind, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8VkSyfV95JJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Save the tokenizers to be used for prediction\n",
        "with open('text_tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(x_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "files.download('text_tokenizer.pickle')\n",
        "with open('summary_tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(y_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "files.download('summary_tokenizer.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZFBBrUUWJd6",
        "colab_type": "code",
        "outputId": "962b12bf-f833-4342-e54d-912d7769e49c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        }
      },
      "source": [
        "#from keras import backend as K\n",
        "from tensorflow.keras import backend as K\n",
        "K.clear_session()\n",
        "\n",
        "latent_dim = 300\n",
        "embedding_dim=100\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_text_len,))\n",
        "\n",
        "#embedding layer\n",
        "enc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n",
        "\n",
        "#encoder lstm 1\n",
        "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
        "\n",
        "#encoder lstm 2\n",
        "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "\n",
        "#encoder lstm 3\n",
        "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "#embedding layer\n",
        "dec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
        "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
        "\n",
        "# Attention layer\n",
        "attn_layer = AttentionLayer(name='attention_layer')\n",
        "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
        "\n",
        "# Concat attention input and decoder LSTM output\n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "#dense layer\n",
        "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "# Define the model \n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.summary() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 50)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 50, 100)      853000      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 50, 300), (N 481200      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 50, 300), (N 721200      lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 100)    175400      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, 50, 300), (N 721200      lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, None, 300),  481200      embedding_1[0][0]                \n",
            "                                                                 lstm_2[0][1]                     \n",
            "                                                                 lstm_2[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "attention_layer (AttentionLayer ((None, None, 300),  180300      lstm_2[0][0]                     \n",
            "                                                                 lstm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concat_layer (Concatenate)      (None, None, 600)    0           lstm_3[0][0]                     \n",
            "                                                                 attention_layer[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, None, 1754)   1054154     concat_layer[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 4,667,654\n",
            "Trainable params: 4,667,654\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7wg_rqxWTo3",
        "colab_type": "code",
        "outputId": "e43876fb-5d76-45c5-85d0-fc37c106eb27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "#compile and train the model\n",
        "filename = 'model.h5'\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "history=model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,epochs=50,batch_size=128, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 159s 596ms/step - loss: 1.7003 - val_loss: 1.4988\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 158s 594ms/step - loss: 1.4739 - val_loss: 1.4141\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 158s 593ms/step - loss: 1.3813 - val_loss: 1.3373\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 157s 589ms/step - loss: 1.3057 - val_loss: 1.2782\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 157s 591ms/step - loss: 1.2505 - val_loss: 1.2466\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 157s 589ms/step - loss: 1.2080 - val_loss: 1.2189\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 156s 586ms/step - loss: 1.1716 - val_loss: 1.1966\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 155s 583ms/step - loss: 1.1404 - val_loss: 1.1849\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 154s 581ms/step - loss: 1.1140 - val_loss: 1.1735\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 155s 584ms/step - loss: 1.0888 - val_loss: 1.1650\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 155s 584ms/step - loss: 1.0663 - val_loss: 1.1587\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 155s 583ms/step - loss: 1.0450 - val_loss: 1.1541\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 155s 584ms/step - loss: 1.0242 - val_loss: 1.1517\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 155s 583ms/step - loss: 1.0057 - val_loss: 1.1516\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 155s 582ms/step - loss: 0.9885 - val_loss: 1.1498\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 157s 592ms/step - loss: 0.9712 - val_loss: 1.1505\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 155s 584ms/step - loss: 0.9558 - val_loss: 1.1506\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 156s 586ms/step - loss: 0.9397 - val_loss: 1.1533\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 155s 581ms/step - loss: 0.9260 - val_loss: 1.1553\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 155s 584ms/step - loss: 0.9109 - val_loss: 1.1574\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 155s 582ms/step - loss: 0.8981 - val_loss: 1.1574\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 155s 582ms/step - loss: 0.8834 - val_loss: 1.1651\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 155s 582ms/step - loss: 0.8712 - val_loss: 1.1658\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 154s 581ms/step - loss: 0.8584 - val_loss: 1.1728\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 155s 582ms/step - loss: 0.8458 - val_loss: 1.1765\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 156s 585ms/step - loss: 0.8345 - val_loss: 1.1819\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 154s 581ms/step - loss: 0.8234 - val_loss: 1.1846\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 154s 581ms/step - loss: 0.8123 - val_loss: 1.1930\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 155s 581ms/step - loss: 0.8004 - val_loss: 1.1963\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 155s 581ms/step - loss: 0.7905 - val_loss: 1.2055\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 154s 580ms/step - loss: 0.7785 - val_loss: 1.2137\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 155s 582ms/step - loss: 0.7686 - val_loss: 1.2160\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 156s 586ms/step - loss: 0.7581 - val_loss: 1.2223\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 156s 586ms/step - loss: 0.7484 - val_loss: 1.2276\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 156s 586ms/step - loss: 0.7390 - val_loss: 1.2338\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 155s 581ms/step - loss: 0.7316 - val_loss: 1.2356\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 155s 581ms/step - loss: 0.7216 - val_loss: 1.2442\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 155s 582ms/step - loss: 0.7129 - val_loss: 1.2456\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 155s 582ms/step - loss: 0.7059 - val_loss: 1.2551\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 154s 580ms/step - loss: 0.6959 - val_loss: 1.2647\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 155s 582ms/step - loss: 0.6881 - val_loss: 1.2734\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 154s 581ms/step - loss: 0.6814 - val_loss: 1.2750\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 154s 579ms/step - loss: 0.6725 - val_loss: 1.2815\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 154s 579ms/step - loss: 0.6659 - val_loss: 1.2881\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 154s 578ms/step - loss: 0.6582 - val_loss: 1.2898\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 154s 580ms/step - loss: 0.6492 - val_loss: 1.2973\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 154s 580ms/step - loss: 0.6427 - val_loss: 1.3066\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 154s 578ms/step - loss: 0.6357 - val_loss: 1.3084\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 154s 578ms/step - loss: 0.6276 - val_loss: 1.3203\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 154s 577ms/step - loss: 0.6206 - val_loss: 1.3195\n",
            "CPU times: user 3h 11min 11s, sys: 24min 20s, total: 3h 35min 31s\n",
            "Wall time: 2h 9min 56s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MQ1iJrn4qKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save the model\n",
        "model.save(\"seq2seq.h5\")\n",
        "files.download('seq2seq.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf9Ux04HWbHD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dictionary to convert the index to word for target and source vocabulary\n",
        "reverse_target_word_index=y_tokenizer.index_word\n",
        "reverse_source_word_index=x_tokenizer.index_word\n",
        "target_word_index=y_tokenizer.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqG0zeRcvJor",
        "colab_type": "text"
      },
      "source": [
        "**Inference Phase**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGml2fHRWgqu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encode the input sequence to get the feature vector\n",
        "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "# Decoder setup\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n",
        "\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb2= dec_emb_layer(decoder_inputs) \n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "#attention inference\n",
        "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "    [decoder_outputs2] + [state_h2, state_c2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssvjwpR8WiiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "    \n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    \n",
        "    # Populate the first word of target sequence with the start word.\n",
        "    target_seq[0, 0] = target_word_index['sostok']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "      \n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "        \n",
        "        if(sampled_token!='eostok'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "        # Exit condition: either hit max length or find stop word.\n",
        "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TDBdJhgWlJj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seq2summary(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
        "            newString=newString+reverse_target_word_index[i]+' '\n",
        "    return newString\n",
        "\n",
        "def seq2text(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if(i!=0):\n",
        "            newString=newString+reverse_source_word_index[i]+' '\n",
        "    return newString"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7H6jxgYWnW5",
        "colab_type": "code",
        "outputId": "a9c4fb87-bc01-47fd-9739-fe98f13f8425",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Testing\n",
        "for i in range(0,100):\n",
        "    print(\"Review:\",seq2text(x_tr[i]))\n",
        "    print(\"Original summary:\",seq2summary(y_tr[i]))\n",
        "    print(\"Predicted summary:\",decode_sequence(x_tr[i].reshape(1,max_text_len)))\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review: nobody beat dunkin donuts coffee addition vanilla make coffee palatable since starbucks name figured worth try could really taste vanilla overall coffee impress either bad good sort sat maybe coffee making skills know vanilla flavor would come \n",
            "Original summary: not impressed \n",
            "Predicted summary:  not fan\n",
            "\n",
            "\n",
            "Review: fantastic coffee fraction price freeze dried keurig cup made company tasteless coffee bean matter name coffee cannot imagine paying dollar cup donut house cup made folgers joke \n",
            "Original summary: great coffee \n",
            "Predicted summary:  great coffee\n",
            "\n",
            "\n",
            "Review: bought first ones seen green veggies good luck consistency organic puffs past hate chemicals gerber puffs best worlds daughter reaches others dissolve quickly pronounce ingredients \n",
            "Original summary: best puffs ever \n",
            "Predicted summary:  great puffs\n",
            "\n",
            "\n",
            "Review: coconut water years back recall tasting like bad could even swallow one pack try add pineapple juice one containers remaining see makes difference wow \n",
            "Original summary: absolutely disgusting \n",
            "Predicted summary:  tastes like plastic\n",
            "\n",
            "\n",
            "Review: favorite flavor love chips favorite flavor barbecue sea salt vinegar sold continue buy \n",
            "Original summary: good chips \n",
            "Predicted summary:  pop chips are great\n",
            "\n",
            "\n",
            "Review: got taste test bubble fizz taste chalk like bad sell taste sad \n",
            "Original summary: not very good \n",
            "Predicted summary:  not good\n",
            "\n",
            "\n",
            "Review: seriously dog sent first birthday gift st time seconds including eating treats moved piece nose ate treat one right paw ate treat one left paw ate treat moved last piece nose ate treat pretty entertaining slow treat eating show durable try kyjen harder puzzles \n",
            "Original summary: my dog is \n",
            "Predicted summary:  my dog loved it\n",
            "\n",
            "\n",
            "Review: quite sure expect sauce arrived realized less dipping sauce sandwich spread consistency similar salad dressing certainly flavor jalapeno without hot taste usually associate jalapeno found nice put sandwich added lot flavor think people like unless looking hot part jalapeno recommended \n",
            "Original summary: good not spicy \n",
            "Predicted summary:  not bad\n",
            "\n",
            "\n",
            "Review: july august great business people keep buying worth pay container figure \n",
            "Original summary: great product price increase \n",
            "Predicted summary:  great product\n",
            "\n",
            "\n",
            "Review: found brand fact dark chocolate made far best keurig continue order particular brand \n",
            "Original summary: best ever \n",
            "Predicted summary:  best chocolate ever\n",
            "\n",
            "\n",
            "Review: thought name attached would wow good average coffee nothing spectacular \n",
            "Original summary: decent coffee but \n",
            "Predicted summary:  not good\n",
            "\n",
            "\n",
            "Review: husband every morning cup coffee decaf girl pinch grab one ok really good \n",
            "Original summary: my husband every morning cup \n",
            "Predicted summary:  great decaf\n",
            "\n",
            "\n",
            "Review: us like licorice one hates one says ok pretty much like regular licorice cool flavors organic year old hated gave dog year said yummy better licorice grownups split like twizzlers like think fancy twizzlers \n",
            "Original summary: hit or miss family is \n",
            "Predicted summary:  hit or miss family\n",
            "\n",
            "\n",
            "Review: loves eating toy treat month still good shape money \n",
            "Original summary: good for feeding \n",
            "Predicted summary:  puppy loves it\n",
            "\n",
            "\n",
            "Review: lowrey microwave pork rinds received unacceptable ordered bags bags popped bag rinds popped amount money paid could gone store bought bag already popped gotten full bag feel popped least half bag understand everyone counting pennies also think consumer get something money really enjoy product probably order reason \n",
            "Original summary: microwave pork rinds review \n",
            "Predicted summary:  love these\n",
            "\n",
            "\n",
            "Review: mother law adult children wife tried quite frankly thought plain bad bad bad could taste liquid smoke resemble jalapeno flavor really disappointed still go taco bell even news said beef tacos like taste get people gave product star rating sorry panel experts say way \n",
            "Original summary: expected better \n",
            "Predicted summary:  not fan\n",
            "\n",
            "\n",
            "Review: buying year love best wake coffee tried try sorry \n",
            "Original summary: the best cups you can buy \n",
            "Predicted summary:  great coffee\n",
            "\n",
            "\n",
            "Review: click tastes great gives mid morning afternoon boost coffee house desk \n",
            "Original summary: love this stuff \n",
            "Predicted summary:  click\n",
            "\n",
            "\n",
            "Review: like jelly bellys think appreciate belly flops surprise flavors get worth every penny opinion size shape affect flavor still taste great \n",
            "Original summary: best flops ever \n",
            "Predicted summary:  awesome\n",
            "\n",
            "\n",
            "Review: say fallen love bb bit understatement taste smell best coffee machine brewed loved sampler pack really great try different things though absolutely love strong tastes amazing wakes makes getting door bit easier downside reach end cup highly recommend \n",
            "Original summary: the morning just got bit better \n",
            "Predicted summary:  just what wanted\n",
            "\n",
            "\n",
            "Review: bars particularly cranberry almond ones best darn snack bars around period buy em box pace get em locally get em \n",
            "Original summary: wow \n",
            "Predicted summary:  delicious\n",
            "\n",
            "\n",
            "Review: hazelnut french vanilla single pods senseo great flavorful smell enticing would order hope come additional flavors future \n",
            "Original summary: senseo coffee from amazon \n",
            "Predicted summary:  senseo coffee pods\n",
            "\n",
            "\n",
            "Review: healthier version star whats million things make awesome protein shake love coffee love click \n",
            "Original summary: love love love this stuff \n",
            "Predicted summary:  love this stuff\n",
            "\n",
            "\n",
            "Review: product received exactly needed problem shipping ups broke bottles company purchased items called tell thank stated would overnight new bottles also refund original shipping took week receive replacements refund \n",
            "Original summary: shipping \n",
            "Predicted summary:  damaged cans\n",
            "\n",
            "\n",
            "Review: disappointed made many things mix several conventional recipes successfully tried several recipes bag pamela website avid baker seriously loss regular purpose flour honestly say mix signed subscribe save receiving regular basis \n",
            "Original summary: my first gf purchase \n",
            "Predicted summary:  great mix\n",
            "\n",
            "\n",
            "Review: disappointed product watered one serving drops big mug like drops even drops seemed like less serving used previous brand like water like glue took drop per serving premium price impressed \n",
            "Original summary: down \n",
            "Predicted summary:  tastes like plastic\n",
            "\n",
            "\n",
            "Review: used get white sage wild mint tea love hint raw honey wanted give tea try disappointed tea tasteless expected \n",
            "Original summary: tasteless \n",
            "Predicted summary:  disappointed\n",
            "\n",
            "\n",
            "Review: sampler pretty good selection flavored coffee least desirable flavors prevalent im sure would include samples highly rated coffee samples low rated coffee maybe trying clear coffee could sell otherwise would put samples every coffee customer could sample variety give rating stars good flavors included found couple new favorites \n",
            "Original summary: decent selection but could be better \n",
            "Predicted summary:  great selection but packaging could be\n",
            "\n",
            "\n",
            "Review: make every morning espresso machine delicious definitely way go drink espresso every day \n",
            "Original summary: great espresso \n",
            "Predicted summary:  great espresso\n",
            "\n",
            "\n",
            "Review: hoping would like healthy would easy take camping dinner would eat kept spitting gave dogs loved sure healthy \n",
            "Original summary: did not love it \n",
            "Predicted summary:  the only food for you\n",
            "\n",
            "\n",
            "Review: cats usually get green fancy feast shredded varieties green stuff elegant cat food brown petite cuisine cat food something dove much watery meat packed fancy suffering loose bowels ever since started test cans going one every three days see handle green stuff two days report back \n",
            "Original summary: more watery than expected cats are not it well \n",
            "Predicted summary:  like beware\n",
            "\n",
            "\n",
            "Review: expecting something sweet almost syrupy like slightly watered version coconut milk shock much lighter taste hint coconut rather overwhelming flavor really enjoyed satisfying really sweet especially good icy icy cold real unexpected taste summer day \n",
            "Original summary: delicious but definitely \n",
            "Predicted summary:  very good\n",
            "\n",
            "\n",
            "Review: coffee strong bitter also priced right believe product goes help \n",
            "Original summary: good strong coffee \n",
            "Predicted summary:  good coffee\n",
            "\n",
            "\n",
            "Review: beneful soft chunks hard chunks would need look elsewhere dog could eat hard dog food though beef mentioned package mention harder find ingredients list however preferred dog though matter human also appetizing look \n",
            "Original summary: mixed bag but dog it \n",
            "Predicted summary:  dog loved it\n",
            "\n",
            "\n",
            "Review: selection received perfect husband thrilled gift would definitely business vendor \n",
            "Original summary: very pleased with my order \n",
            "Predicted summary:  great gift\n",
            "\n",
            "\n",
            "Review: thought cookie decent tasting store bought cookie thus star rating texture bit soft cookie tendency crumble tried cookie room temperature cold heated microwave remained good store bought others like better favorite thing cookie really probably easily package \n",
            "Original summary: decent for store bought cookie but not great \n",
            "Predicted summary:  cookies\n",
            "\n",
            "\n",
            "Review: daughter never sucker trying suckers loves even though cannot figure hold stick sucker tried one yesterday cannot tell organic ones bad thing pricey buy time take good eat \n",
            "Original summary: yummy suckers \n",
            "Predicted summary:  great suckers\n",
            "\n",
            "\n",
            "Review: found coffee costco count purchased box bought went back today buy discontinued whole italian family loves fog chaser coffee using flavors mostly paul newman like much better rich strong wonderful taste appeals us ordering count amazon happy find available \n",
            "Original summary: fog chaser cup \n",
            "Predicted summary:  fog chaser coffee\n",
            "\n",
            "\n",
            "Review: second favorite tea love enjoy stronger spice flavors great tea \n",
            "Original summary: great tea \n",
            "Predicted summary:  great tea\n",
            "\n",
            "\n",
            "Review: solid product gu usual prefer brand gel packs espresso love one better flavors \n",
            "Original summary: good stuff \n",
            "Predicted summary:  good stuff\n",
            "\n",
            "\n",
            "Review: fenugreek seed helps digestion new mother breastfeeding digestion became evenly time hungry food later without taking fenugreek take supplement regularly however gave sister report good news product works wonders mothers babies new mother want abundance milk baby feedings recommend product brand \n",
            "Original summary: wonder full supplement \n",
            "Predicted summary:  works for me\n",
            "\n",
            "\n",
            "Review: like light medium blends darker roasts give week cup coffee really bitter aftertaste jet fuel completely different type coffee gives exactly want morning since extra bold body darker roast also available large packages \n",
            "Original summary: perfect cup in the morning \n",
            "Predicted summary:  good cup of coffee\n",
            "\n",
            "\n",
            "Review: dog loves chew figured product created specifically dogs love chew would perfect rocky actually ate bone proceeded throw pieces nylabone next morning horrible dog agressive chewer purchase product \n",
            "Original summary: not for really chewer \n",
            "Predicted summary:  not for my dog\n",
            "\n",
            "\n",
            "Review: dog treat use future calories healthy loves \n",
            "Original summary: very pleased with mini naturals \n",
            "Predicted summary:  dog loves them\n",
            "\n",
            "\n",
            "Review: previously couple flavors switch beverages liked good bit one bad certainly kiwi flavor prominent really try distinguish kiwi flavor berry tasted since would like flavors well expecting would like one well even bother finishing drink people might like better found sweet worthy drinking \n",
            "Original summary: mediocre beverage \n",
            "Predicted summary:  not too bad\n",
            "\n",
            "\n",
            "Review: first bought batch home brewed beer found tastes great substituted many recipes continuing use product \n",
            "Original summary: great product so many different uses \n",
            "Predicted summary:  great product\n",
            "\n",
            "\n",
            "Review: ordered husband said okay likes green mountain brand better next time order green mountain brand \n",
            "Original summary: good but liked green mountain better \n",
            "Predicted summary:  too sweet\n",
            "\n",
            "\n",
            "Review: honestly know really get overly concerned ph balance electrolytes keep handy husband long mile bike trips us like tastes would rather drink gatorade also know silky smooth taste tastes like water wanted \n",
            "Original summary: do not know about health issues but tastes fine \n",
            "Predicted summary:  do not know about health issues\n",
            "\n",
            "\n",
            "Review: love delicious good source protein like eating dessert good recommend product anyone wants protein snack \n",
            "Original summary: excelent bar \n",
            "Predicted summary:  kind plus cranberry almond bars\n",
            "\n",
            "\n",
            "Review: wild first grew pepper bit overpowering first bites sneaks however eating several times enjoyed complaint plastic packaging although convenient wasteful environmentally unfriendly please recycle canisters \n",
            "Original summary: fairly good \n",
            "Predicted summary:  nothing special\n",
            "\n",
            "\n",
            "Review: eating chinese food products like playing didnt know donate food bank \n",
            "Original summary: product of china \n",
            "Predicted summary:  pretty good\n",
            "\n",
            "\n",
            "Review: box makes worth money tea ok teapot cute small \n",
            "Original summary: really cool box \n",
            "Predicted summary:  box\n",
            "\n",
            "\n",
            "Review: product works perfectly first began using product tons moths caught traps find moths traps house highly recommend pantry pest anyone problems pantry moths etc \n",
            "Original summary: no more pantry \n",
            "Predicted summary:  pantry pest trap\n",
            "\n",
            "\n",
            "Review: really love olive chips brought taste home actually see olives quality exact opposite processed chips taste munched sample bag mere seconds decided buy full size package amazon get stars instead reflect high cost per ounce \n",
            "Original summary: it is an chip \n",
            "Predicted summary:  love these chips but not the best\n",
            "\n",
            "\n",
            "Review: love green mountain nantucket coffee every thing hoped would taste aroma awesome certainly purchase try might love get coffee amazon easy delivery quick pamela \n",
            "Original summary: green mountain nantucket coffee \n",
            "Predicted summary:  great coffee\n",
            "\n",
            "\n",
            "Review: time opened box containing sweetener already leaking seal believe products meant consumption well packaged made airtight possible since mio already leaking felt safe use cause knows leaked makers food items take time make sure products well sealed \n",
            "Original summary: not well sealed \n",
            "Predicted summary:  it does not work\n",
            "\n",
            "\n",
            "Review: gotten older tolerance acid seems sometimes cannot tolerate coffee love darker better tried brands low acid coffee found weak tasteless puroast coffee best smooth great deep flavor worked well keurig also absolutely bitterness acidic issues recommend trying issues buying \n",
            "Original summary: best low acid coffee \n",
            "Predicted summary:  smooth\n",
            "\n",
            "\n",
            "Review: cannot say enough good things snackwell fudge drizzle caramel popcorn remember snackwell products first introduced good real cookies come long way packs decent serving popcorn plenty crunchy caramel chocolate overwhelming great balance already purchased \n",
            "Original summary: highly addictive \n",
            "Predicted summary:  good snack\n",
            "\n",
            "\n",
            "Review: high quality coffee particularly well suit bold coffee drinkers taste strong flavorful bitterness generally morning coffee drinker since afternoon coffee upsets stomach however tolerate newman coffee afternoon well balanced favor acidity great value cup package \n",
            "Original summary: newman best coffee \n",
            "Predicted summary:  great coffee\n",
            "\n",
            "\n",
            "Review: stuff looks better dinner last night kitty mr absolutely loves stuff actual pieces seafood calamari shrimp label keep trying brand keep coming flavors picky poof loves weruva ounce cans \n",
            "Original summary: holy cat food \n",
            "Predicted summary:  cat food\n",
            "\n",
            "\n",
            "Review: male dog developed anal infections vet recommended adding canned pumpkin meals fiber unfortunately canned pumpkin seasonal item local stores excited find item available via subscribe save occasionally found dented cans frequently severely enough cannot use anyway dogs enjoy \n",
            "Original summary: fiber for dogs \n",
            "Predicted summary:  fiber for our dog\n",
            "\n",
            "\n",
            "Review: rich great tasting dark coffee bitter good coffee enjoy dark richer tasting coffee \n",
            "Original summary: great taste \n",
            "Predicted summary:  great coffee\n",
            "\n",
            "\n",
            "Review: didnt like flavor sweet using one scoop putting regular coffee use \n",
            "Original summary: didnt click for me \n",
            "Predicted summary:  hour energy pomegranate\n",
            "\n",
            "\n",
            "Review: carousel awesome holds many hardly ever refill addition doesnt take much counter space taller others \n",
            "Original summary: awesome \n",
            "Predicted summary:  great storage bags\n",
            "\n",
            "\n",
            "Review: suspect unique saying two main concerns drink would either sweet possibly medicinal worry turns pleasant fruity taste subtle sugary overly artificial likewise got carbonation right bubbly enough light one complaint small \n",
            "Original summary: just right \n",
            "Predicted summary:  nice alternative to soda\n",
            "\n",
            "\n",
            "Review: product fantastic anyone diet watching fat intake give try could believe delicious real peanuts mix water get creamy delicious peanut butter miss anything except calories \n",
            "Original summary: the best \n",
            "Predicted summary:  pb\n",
            "\n",
            "\n",
            "Review: first saw reviews product got excited purchased two boxes got disappointed realize least part protein soy general tend stay away soy products found taste rather meh awful overall think buy \n",
            "Original summary: meh not overly by this product \n",
            "Predicted summary:  meh\n",
            "\n",
            "\n",
            "Review: breastfeeding mom started tea wednesday noticed difference following friday night amazing stuff suggest buying biggest pack since drink times day \n",
            "Original summary: amazing \n",
            "Predicted summary:  amazing\n",
            "\n",
            "\n",
            "Review: think great sign package cookies house people ends tossed finished horrible would want spend money reccomend first fairly small thin smaller sizes desserts bad idea gooey found way dry really see supposed gooey \n",
            "Original summary: small thin and came more dry than gooey \n",
            "Predicted summary:  not so good\n",
            "\n",
            "\n",
            "Review: like bags cheapest could find anywhere net used bags much firm sturdy however price better also honey bags hold enormous amount milk store oz one bags without difficulty problems leaking breaking keep mind free standing bags like ones \n",
            "Original summary: milk bags \n",
            "Predicted summary:  works great\n",
            "\n",
            "\n",
            "Review: awful things put mouth taste weird texture made feel sick never buy \n",
            "Original summary: surprised \n",
            "Predicted summary:  bad taste\n",
            "\n",
            "\n",
            "Review: dog enough knows one really goes nuts hitch order length time getting thank much linda would recommend ordering anyone greenies part dogs lives nearly thirteen years look forward one everyday ordered amazon everything went smoothly \n",
            "Original summary: my dog would like to give them stars \n",
            "Predicted summary:  my dog loves this stuff\n",
            "\n",
            "\n",
            "Review: says energy drink coffee unlike coffee carry class take need \n",
            "Original summary: hour extra strength berry \n",
            "Predicted summary:  hour energy berry\n",
            "\n",
            "\n",
            "Review: stash teas one favorites green white tea healthy delicious hot cold enjoy cup hot tea morning day relaxing evening keep jug iced green white tea refrigerator year round \n",
            "Original summary: fabulous everything \n",
            "Predicted summary:  great tea\n",
            "\n",
            "\n",
            "Review: bold robust coffee new favorite rich dark bitter perfect morning wake \n",
            "Original summary: love it love it love it \n",
            "Predicted summary:  bold and smooth\n",
            "\n",
            "\n",
            "Review: normally drink donut shop cup tried pretty much every cup found stores mom pretty much forced check im glad taste strong bitter become go cup need coffee pick morning \n",
            "Original summary: best tried so far \n",
            "Predicted summary:  good strong cup of coffee\n",
            "\n",
            "\n",
            "Review: smooth creamy nutty slight hint chocolate makes great crema beautiful shot espresso \n",
            "Original summary: lavazza super crema is everything you hope for in italian espresso \n",
            "Predicted summary:  best espresso\n",
            "\n",
            "\n",
            "Review: really nice coffee come love every morning starts day happy cups easy pass price slowly time types coffee \n",
            "Original summary: delicious \n",
            "Predicted summary:  love this coffee\n",
            "\n",
            "\n",
            "Review: drinking one two cups senseo medium roast coffee pods almost everyday two years quick easy mess reliable taste pod coffee outstanding convenient simple need run house morning tried senseo dark roast bitter specialty pods waste money found none tasted good medium roast \n",
            "Original summary: quick morning brew \n",
            "Predicted summary:  great morning coffee\n",
            "\n",
            "\n",
            "Review: shipping quick coffee strong exactly reviews said bitter ordering soon \n",
            "Original summary: good stuff \n",
            "Predicted summary:  good stuff\n",
            "\n",
            "\n",
            "Review: bought two bones small dogs believe enjoyed first later day stopped playing dinner time noticed eating much normally would guess toys might hard small dogs days later brother brought beagle loved chicken flavor noticeable take package would recommend bones medium large breed dog \n",
            "Original summary: like rocks \n",
            "Predicted summary:  good product but worth the money\n",
            "\n",
            "\n",
            "Review: used coffee trying others seem go back nice cup morning coffee price good \n",
            "Original summary: very nice cup of morning coffee \n",
            "Predicted summary:  great coffee\n",
            "\n",
            "\n",
            "Review: stopped using sugar free coffee syrups switched naturally calorie free flavored stevia want use artificial sweeteners anymore good option stevia leave slight taste enough use product try \n",
            "Original summary: great natural replacement for sugar free \n",
            "Predicted summary:  great natural sweetner\n",
            "\n",
            "\n",
            "Review: senseo brand espresso yummo especially senseo coffee maker leaves nice foam top yummo \n",
            "Original summary: yummo \n",
            "Predicted summary:  not as good as the regular thing\n",
            "\n",
            "\n",
            "Review: many flavors hard pick one delicious hard pick favorite easy use coffee ready flash \n",
            "Original summary: hard to \n",
            "Predicted summary:  delicious\n",
            "\n",
            "\n",
            "Review: impressed switch black cherry watermelon strawberry caught surprise big fan watermelon flavors never quite real thing refreshing taste carbonation right liked flavor mix right sweetness good watermelon strawberry ratio plus sugar free aspartame free sweetened natural juices great low calorie drink perfect kids grownups \n",
            "Original summary: surprisingly refreshing \n",
            "Predicted summary:  refreshing drink\n",
            "\n",
            "\n",
            "Review: love ginger dressing get japanese restaurants salad love fresh ginger added juicer machine freshly made veggie juices hot tea utterly disgusting feel burn palate mean temperature bad enough lingers mouth yes taste lemon slightly ginger knocks taste buds recommended \n",
            "Original summary: disgusting and is on the \n",
            "Predicted summary:  disgusting\n",
            "\n",
            "\n",
            "Review: puroast deep coffee smell first opened package give rich smell brewed comforting scent says wake taste ok best advertised bite back cause acid reflux heartburn brewed standard drip machine sticking usual brand coffee rather switching puroast \n",
            "Original summary: will not be to \n",
            "Predicted summary:  starbucks aftertaste\n",
            "\n",
            "\n",
            "Review: read poor reviews concerned ordering coffee mix several reviews complained mix pleased receive order get duplicate coffees ones maker different flavors full decaf tea teas see decaf seller may reading reviews happy order \n",
            "Original summary: very pleased with order \n",
            "Predicted summary:  not as good as the first time\n",
            "\n",
            "\n",
            "Review: tasty sweet found gu packets easy ingest effective energy replacement tools endurance sports vanilla bean flavour especially easy swallow without lingering aftertaste used extensively never problem like similar brands buying box economical \n",
            "Original summary: the best of the energy \n",
            "Predicted summary:  good taste\n",
            "\n",
            "\n",
            "Review: daughter well formula organic gives us bit piece mind since cant breastfeed anymore love wont changing \n",
            "Original summary: we love it \n",
            "Predicted summary:  good stuff\n",
            "\n",
            "\n",
            "Review: bought bags friend told great well worth money agree wonderful pumping lot never problem leaking falling apart thank honeysuckle making great product \n",
            "Original summary: great product \n",
            "Predicted summary:  great product\n",
            "\n",
            "\n",
            "Review: drinking senseo douwe medium decaf coffee many years even back machine case something fails never pour dash whipping cream sit back enjoy \n",
            "Original summary: better than any latte \n",
            "Predicted summary:  coffee\n",
            "\n",
            "\n",
            "Review: alot research decided purchase coconut oil happy thus far happy purchase word advice going ingest coconut oil go youtube search green regimen green smoothie video blog put coconut oil smoothies working greatly good luck \n",
            "Original summary: great buy \n",
            "Predicted summary:  great oil\n",
            "\n",
            "\n",
            "Review: organic fruit sugar great taste like everyone family loved dried fruit twists added sugar artificial ingredients makes great snack desire eat health like great taste \n",
            "Original summary: great fruit bar \n",
            "Predicted summary:  great tasting cereal\n",
            "\n",
            "\n",
            "Review: bought two traps last month tried get set ground spent hour trying get trap set would stick taken time try get work since put seems slowed mole activity yard definately try get set mole activity picks back right collecting dust shed \n",
            "Original summary: cannot get it set \n",
            "Predicted summary:  works\n",
            "\n",
            "\n",
            "Review: received package opened immediately read much wonderful mix skeptical made mine little thicker directions recommend came perfect like mine crispy crispy tast fantastic would highly recommend mix anyone like waffles \n",
            "Original summary: fabulous waffles \n",
            "Predicted summary:  first time from first\n",
            "\n",
            "\n",
            "Review: ordered expecting slightly bigger toy considering large toy toy great small dogs like something like small dog head lasted one day gone disappointed considering alot good say nylabone products hope see better bigger dinosaur toy soon \n",
            "Original summary: size for size large dogs \n",
            "Predicted summary:  dogs love it\n",
            "\n",
            "\n",
            "Review: love newman organics anything let alone newman organics dark chocolate candy bars tasty melt nicely mouth really need chocolate love fact organic really love fair trade well labor delicious chocolate candy bar \n",
            "Original summary: delicious \n",
            "Predicted summary:  delicious\n",
            "\n",
            "\n",
            "Review: oz larger find pet stores less expensive shipped day ordered arrived days later prime \n",
            "Original summary: fast shipment \n",
            "Predicted summary:  great taste\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}