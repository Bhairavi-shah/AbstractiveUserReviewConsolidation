\documentclass[11pt]{report}
\usepackage{csvsimple}
\usepackage{pgfplotstable}
\usepackage[utf8]{inputenc}
\usepackage{longtable}
\setlength{\tabcolsep}{5pt}

\begin{filecontents*}{ls.csv}
slno;title;author;year;technique;advantages;disadvantages
1;A Multi-view Abstractive Summarization Model Jointly Considering Semantics and Sentiment;Moye Chen, Lei Li, Wei Liu;2018;Encoder-Decoder Recurrent Neural Network;Multi-view model extracts sentiment features automatically. Low dimension vector for SE proved to be a powerful representation since sentiment categories are not complex.This model was proven to be an improvement to existing baseline systems;Sentiment labeling should be done manually since there is no dataset with labeled sentiment. More attention should be given to intra-relationship of words. Sentiment factor was not considered while computing loss function.
2;Generating Abstractive Summaries Using Sequence to Sequence Attention Model;Tooba Siddiqui, Jawwad Ahmed Shamsi;2018;Sequence to Sequence Attention Model;The rouge score of the temporal attention model is higher as compared to the rouge score of the global attention model.;Computational cost for summarisation increases with documents, layers and iterations.
3;Abstractive Multi-document Summarization;Ranjitha N S, Dr. Jagadish S Kallimani;2017;Linear Optimization Method, Semantic Information Text Approach;Repetition avoided using agglomerative hierarchical clustering. This approach is applicable with respect to any domain and requires no interventions of human experts. Provides PAS representation with high salience value.;It fails for words with opinions.
4;Extracting Aspects and Mining Opinions in Product Reviews Using Supervised Learning Algorithm;A.Jeyapriya, C.S. Kanimozhi Selvi;2015;Sentiment Analysis/Aspect Based Opinion Mining;Positive and negative opinions are separated from the documents. Sentiment orientation algorithm is used to find the probability of positive and negative opinions. Good accuracy.;New sentences are not created.
5;An Abstractive Summarizer Based on Improved Pointer-Generator Network;Wenbo Nie, Wei Zhang, Xinle Li, Yao Yu;2019;Pointer-Generator Model, Attention Mechanism;It has a higher Rouge score than the basic pointer-generator model. It can handle out-of-vocabulary(OOV) words.;Repetition is not fully eliminated.
6;Dual Encoding for Abstractive Text Summarization;Kaichun Yao, Libo Zhang, Dawei Du, Tiejian Luo, Lili Tao, Yanjun Wu;2018;Dual Encoding Model;Uses an enhanced repetition avoid mechanism which improves the quality of the generated summary. The secondary encoding is more likely to fulfil a fine and selective encoding to help decoder produce better summary. More suitable for long sequence generation tasks.;A large decoding length makes the secondary encoder out of function. A small decoding length is not able to capture enough information and increases computational cost due to more secondary encoding operation.
7;Multi-document Abstractive Summarization Based on Predicate Argument Structure;Alshaina S, Ansamma John, Aneesh G Nath;2017;Predicate Argument Structure;Better computation time than existing systems. Abstractive text summarization produces highly meaningful, knowledge rich and less redundant summary. Provide a viable solution than other algorithms.;Feature selection is made randomly.
8;Evaluation of Automatic Text Summarization Based on Human Summaries;Farshad Kiyoumarsi;2014;Fuzzy Method, Vector Approach;Quality of summaries produced by humans and by using automatic summarization was comparable. Automatic more efficient when volume of data increases. Most of the time, readers are able to understand the summaries using their common sense and make the summaries coherent in their mind.;Automatically generated summaries are not coherent and intelligent as human summaries, since humans can think and decide on the best option.
9;Clustered Genetic Semantic Graph Approach for Multi-document Abstractive Summarization;Atif Khan, Naomie Salim, Haleem Farman;2016;Clustered Genetic Semantic Graph Approach;The semantic similarity measures assists in detecting redundancy by capturing semantically equivalent predicate argument structures thereby improving results. Does not require any intervention of human experts.;The proposed approach assumes semantic structure of sentence. The impact of Cross-Document Structural Theory (CST) relations for multi-document abstractive summarization is not considered.
10;Integrating Extractive and Abstractive Models for Long Text Summarization;Shuai Wang, Xiang Zhao, Bo Li, Bin Ge, Daquan Tang;2017;Graph Model, Recurrent Neural Network;Integration of the state-of-art models leverages the advantages of both extractive and abstractive summarization methods, and achieves significant performance improvements when dealing with long text. A real world dataset from financial domain for long text summarization is used.;Requires a large-scale structured training data. Not capable of multi-document text summarization.
11;Multi Document Abstractive Summarization using ILP Based Multi Sentence Compression;Siddhartha Banerjee, Prasenjit Mitra, Kazunari Sugiyama;2015;Inter Linear Programming Model(ILP);Achieves promising results on informativeness and readability. Log probability score is assigned as an indicator of linguistic quality. ILP is a novel methodology to be used.;Requires manual evaluation
12;A Neural Attention Model for Abstractive Sentence Summarization;Alexander M. Rush, Sumit Chopra, Jason Weston;2015;Neural Network;Can easily scale to train on a large amount of data. Can be trained directly on any document-summary pair.;Repeating semantic elements. Altering semantic roles. Improper generalization.
13;Multi-Document Abstractive Summarization using Chunk-Graph and Recurrent Neural Network;Jianwei Niu, Huan Chen, Qingjuan Zhao, Limin Sun, Mohammed Atiquzzaman;2017;Chunk Graph and Recurrent Neural Network;Using CG instead of words as basic unit greatly reduces the graph size. Smaller graph size can reduce the computation load effectively.;CG filters the sentence paths in low linguistic quality.
14;Sequence Generative Adversarial Network for Long Text Summarization;Hao Xu, Yanan Cao, Ruipeng Jia, Yanbing Liu, Jianlong Tan;2018;Generative Adversarial Network(GAN);Discriminator model provides additional improvement in performance. It is more prominent in effective summarization of long source text.;The generated summary still contains repeating phrases. This model uses supervised learning, which rely on high quality datasets which is scarce.
15;Multiple Text Document Summarization System using Hybrid Summarization Technique;Harsha Dave, Shree Jaswal;2015;Hybrid Technique, WordNet Ontology;The generated abstractive summary is in well-compressed, grammatically correct and human readable format.;As the size of the document increases, system will take more time to generate summary.
16;Multi-Layered Sentimental Analytical Model For Product Review Mining;Jagbir Kaur, Meenakshi Bansal;2016;Sentiment Analysis/Opinion Mining;The proposed system has very high accuracy(99 percent). It is efficient and accurate in terms of assessed parameters.;Does not consider compression error.
\end{filecontents*}

\begin{document}

%\csvautotabular[respect all]{ls.csv}

\begin{center}
\begin{longtable}{| m{1.1em} | m{6.5em} | m{5em} | m{2em} | m{4.8em} | m{7.5em} | m{7.5em} |}
    \hline
    \bfseries Sl. No & \bfseries Title & \bfseries Author & \bfseries Year & \bfseries Technique & \bfseries Advantages & \bfseries Disadvantages \csvreader[head to column names, separator=semicolon]{ls.csv}{}
    {\\\hline\slno & \title & \author & \year & \technique & \advantages & \disadvantages }
    \\\hline
    \caption{Literature Survey}
\end{longtable}
\end{center}
\end{document}