{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Module2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMRgDgLtJrXDOpNwFhN/aa0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhairavi-shah/AbstractiveUserReviewConsolidation/blob/master/Module2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fowfgXu_vqKa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "e6061c31-ec16-4db6-97ba-fff71ec3cea2"
      },
      "source": [
        "%%time\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import pandas as pd\n",
        "\"\"\"\n",
        "df = pd.read_json(r\"/content/gdrive/My Drive/PROJECT | S7-S8/Data/AMAZON_FASHION_5.json\", lines = True)\n",
        "print(df.head())\n",
        "\n",
        "df.groupby('asin').mean()\n",
        "\n",
        "df2 = (df.loc[df['asin'] == 'B0014F7B98'])\n",
        "print(df2.head())\n",
        "\n",
        "df2.to_json(F\"/content/gdrive/My Drive/PROJECT | S7-S8/Data/B0014F7B98.json\", orient ='index')\n",
        "\n",
        "df3 = pd.read_csv(F\"/content/gdrive/My Drive/PROJECT | S7-S8/Data/twitter_sentiment_train.csv\", engine='python')\n",
        "print(df3.head())\n",
        "\"\"\"\n",
        "\n",
        "# nltk\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from  nltk.stem import SnowballStemmer\n",
        "\n",
        "import re\n",
        "\n",
        "df = pd.read_csv(F\"/content/gdrive/My Drive/PROJECT | S7-S8/Data/training.csv\", engine='python', \n",
        "                  names=[\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"])\n",
        "\"\"\"\n",
        "target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
        "ids: The id of the tweet ( 2087)\n",
        "date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
        "flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
        "user: the user that tweeted (robotickilldozr)\n",
        "text: the text of the tweet (Lyx is cool)\n",
        "\"\"\"\n",
        "\n",
        "pd.set_option('display.max_colwidth',1000)\n",
        "print(df.head())\n",
        "print(df.columns)\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words(\"english\")\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# TEXT CLEANING\n",
        "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
        "\n",
        "def preprocess(text, stem=False):\n",
        "    # Remove link,user and special characters\n",
        "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if token not in stop_words:\n",
        "            if stem:\n",
        "                tokens.append(stemmer.stem(token))\n",
        "            else:\n",
        "                tokens.append(token)\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "df['PreprocessedText'] = df.text.apply(lambda x: preprocess(x))\n",
        "print(df)\n",
        "print(df.columns)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "   target  ...                                                                                                                 text\n",
            "0       0  ...  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
            "1       0  ...      is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\n",
            "2       0  ...                            @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds\n",
            "3       0  ...                                                                      my whole body feels itchy and like its on fire \n",
            "4       0  ...      @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. \n",
            "\n",
            "[5 rows x 6 columns]\n",
            "Index(['target', 'ids', 'date', 'flag', 'user', 'text'], dtype='object')\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "         target  ...                                                       PreprocessedText\n",
            "0             0  ...                           awww bummer shoulda got david carr third day\n",
            "1             0  ...  upset update facebook texting might cry result school today also blah\n",
            "2             0  ...                   dived many times ball managed save 50 rest go bounds\n",
            "3             0  ...                                       whole body feels itchy like fire\n",
            "4             0  ...                                                       behaving mad see\n",
            "...         ...  ...                                                                    ...\n",
            "1599995       4  ...                                          woke school best feeling ever\n",
            "1599996       4  ...                               thewdb com cool hear old walt interviews\n",
            "1599997       4  ...                                        ready mojo makeover ask details\n",
            "1599998       4  ...                   happy 38th birthday boo alll time tupac amaru shakur\n",
            "1599999       4  ...              happy charitytuesday thenspcc sparkscharity speakinguph4h\n",
            "\n",
            "[1600000 rows x 7 columns]\n",
            "Index(['target', 'ids', 'date', 'flag', 'user', 'text', 'PreprocessedText'], dtype='object')\n",
            "CPU times: user 1min 4s, sys: 1.07 s, total: 1min 5s\n",
            "Wall time: 1min 5s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofTblai3O9Kf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0202f8a7-89ca-4d57-dae5-cdb68db0f56a"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "# Encoding string valued columns\n",
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "for column_name in df.columns:\n",
        "  if df[column_name].dtype == object:\n",
        "    df[column_name] = le.fit_transform(df[column_name])\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "\n",
        "# Split target column\n",
        "y = df[['target']]\n",
        "df_traintest = df.drop(columns=['target'])\n",
        "\n",
        "# create training and testing vars\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_traintest, y, test_size=0.2)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1280000, 6) (1280000, 1)\n",
            "(320000, 6) (320000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raQwF4XIOBGo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "8550c694-34ab-414e-c8e8-e196889e15c5"
      },
      "source": [
        "#fit a model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "text_classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
        "text_classifier.fit(X_train, y_train.values.ravel())"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
              "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
              "                       warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ0Byh-kery7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = text_classifier.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "889KBtM-dnou",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "9a1b0fc4-aecf-406d-b811-71118dde5c3e"
      },
      "source": [
        "# Score\n",
        "print(\"Score:\", text_classifier.score(X_test, y_test))\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "print(confusion_matrix(y_test,predictions))\n",
        "print(classification_report(y_test,predictions))\n",
        "print(accuracy_score(y_test, predictions))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score: 0.725825\n",
            "[[102012  57916]\n",
            " [ 29820 130252]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.64      0.70    159928\n",
            "           4       0.69      0.81      0.75    160072\n",
            "\n",
            "    accuracy                           0.73    320000\n",
            "   macro avg       0.73      0.73      0.72    320000\n",
            "weighted avg       0.73      0.73      0.72    320000\n",
            "\n",
            "0.725825\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}